{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "187328a4",
   "metadata": {},
   "source": [
    "## Python code of ML Model Performing Fraud Detection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ed9a9a9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Values:\n",
      " step              0\n",
      "type              0\n",
      "amount            0\n",
      "nameOrig          0\n",
      "oldbalanceOrg     0\n",
      "newbalanceOrig    0\n",
      "nameDest          0\n",
      "oldbalanceDest    0\n",
      "newbalanceDest    0\n",
      "isFraud           0\n",
      "isFlaggedFraud    0\n",
      "dtype: int64\n",
      "VIF Values:\n",
      "           feature         VIF\n",
      "0           const    4.137111\n",
      "1            step    1.003137\n",
      "2          amount    3.771634\n",
      "3   oldbalanceOrg  502.913267\n",
      "4  newbalanceOrig  504.282321\n",
      "5  oldbalanceDest   66.101079\n",
      "6  newbalanceDest   76.200749\n",
      "7         isFraud    1.186855\n",
      "8  isFlaggedFraud    1.002562\n",
      "Random Forest Metrics:\n",
      "Accuracy: 0.9996\n",
      "Precision: 0.9783\n",
      "Recall: 0.5902\n",
      "F1 Score: 0.7362\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from statsmodels.tools.tools import add_constant\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('Fraud.csv')\n",
    "\n",
    "# Check for missing values\n",
    "missing_values = df.isnull().sum()\n",
    "print(\"Missing Values:\\n\", missing_values)\n",
    "\n",
    "# Data Cleaning\n",
    "\n",
    "# Handle missing values\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "df[['oldbalanceOrg', 'newbalanceOrig', 'oldbalanceDest', 'newbalanceDest']] = imputer.fit_transform(df[['oldbalanceOrg', 'newbalanceOrig', 'oldbalanceDest', 'newbalanceDest']])\n",
    "\n",
    "# Handle multicollinearity using VIF\n",
    "X_numeric = df.select_dtypes(include=[np.number])\n",
    "X_const_numeric = add_constant(X_numeric)  # Add a constant column for the intercept term\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data[\"feature\"] = X_const_numeric.columns\n",
    "vif_data[\"VIF\"] = [variance_inflation_factor(X_const_numeric.values, i) for i in range(X_const_numeric.shape[1])]\n",
    "print(\"VIF Values:\\n\", vif_data)\n",
    "\n",
    "# Handle outliers using IQR method\n",
    "def handle_outliers(df, columns):\n",
    "    for column in columns:\n",
    "        Q1 = df[column].quantile(0.25)\n",
    "        Q3 = df[column].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        df = df[(df[column] >= lower_bound) & (df[column] <= upper_bound)]\n",
    "    return df\n",
    "\n",
    "# Specify columns to check for outliers\n",
    "outlier_columns = ['oldbalanceOrg', 'newbalanceOrig', 'oldbalanceDest', 'newbalanceDest']\n",
    "\n",
    "# Handle outliers\n",
    "df = handle_outliers(df, outlier_columns)\n",
    "\n",
    "# Data Preprocessing\n",
    "# Convert categorical variables to numerical using one-hot encoding\n",
    "df = pd.get_dummies(df, columns=['type'], drop_first=True)\n",
    "\n",
    "# Create a new feature indicating if the recipient is a merchant\n",
    "df['isMerchant'] = df['nameDest'].apply(lambda x: 1 if x.startswith('M') else 0)\n",
    "\n",
    "# Drop 'nameDest' and 'nameOrig' columns\n",
    "df = df.drop(['nameDest', 'nameOrig'], axis=1)\n",
    "\n",
    "# Convert data types to ensure compatibility\n",
    "numeric_columns = ['oldbalanceOrg', 'newbalanceOrig', 'oldbalanceDest', 'newbalanceDest']\n",
    "df[numeric_columns] = df[numeric_columns].astype(float)\n",
    "\n",
    "# Split the data into features (X) and target variable (y)\n",
    "X = df.drop('isFraud', axis=1)\n",
    "y = df['isFraud']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Feature Scaling (if needed)\n",
    "numeric_columns = X_train.select_dtypes(include=[np.number]).columns\n",
    "scaler = StandardScaler()\n",
    "X_train[numeric_columns] = scaler.fit_transform(X_train[numeric_columns])\n",
    "X_test[numeric_columns] = scaler.transform(X_test[numeric_columns])\n",
    "\n",
    "# Model Training - Random Forest\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Model Evaluation - Random Forest\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "\n",
    "# Evaluate the Random Forest model\n",
    "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
    "precision_rf = precision_score(y_test, y_pred_rf)\n",
    "recall_rf = recall_score(y_test, y_pred_rf)\n",
    "f1_rf = f1_score(y_test, y_pred_rf)\n",
    "\n",
    "# Display metrics for Random Forest\n",
    "print(\"Random Forest Metrics:\")\n",
    "print(f\"Accuracy: {accuracy_rf:.4f}\")\n",
    "print(f\"Precision: {precision_rf:.4f}\")\n",
    "print(f\"Recall: {recall_rf:.4f}\")\n",
    "print(f\"F1 Score: {f1_rf:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e93eefd",
   "metadata": {},
   "source": [
    "## Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e6d3d0",
   "metadata": {},
   "source": [
    "### 1. Data Cleaning including Missing Values, Outliers, and Multi-collinearity\n",
    "\n",
    "Missing Values: Imputation using the mean strategy for columns 'oldbalanceOrg', 'newbalanceOrig', 'oldbalanceDest', 'newbalanceDest'.\n",
    "Outliers: Outliers are handled using the IQR (Interquartile Range) method for columns 'oldbalanceOrg', 'newbalanceOrig', 'oldbalanceDest', 'newbalanceDest'.\n",
    "Multi-collinearity: Variance Inflation Factor (VIF) is used to detect and handle multicollinearity.\n",
    "\n",
    "### 2. Describe Your Fraud Detection Model in Elaboration\n",
    "\n",
    "The fraud detection model implemented in this code is a Random Forest Classifier. Here's a more detailed breakdown:\n",
    "\n",
    "Random Forest Classifier:\n",
    "\n",
    "A Random Forest is an ensemble learning method that operates by constructing a multitude of decision trees during training and outputs the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees.\n",
    "Each tree is constructed using a subset of the training data and a random subset of features. This randomness introduces diversity among the trees, making the model robust and less prone to overfitting.\n",
    "The final prediction is determined by aggregating the predictions of all the individual trees.\n",
    "Supervised Learning:\n",
    "\n",
    "It is a supervised learning model, meaning it is trained on a labeled dataset where the target variable (in this case, 'isFraud') is known.\n",
    "The model learns patterns and relationships between features and the target variable during the training phase.\n",
    "Feature Engineering:\n",
    "\n",
    "Features used for training the model include both numerical and one-hot encoded categorical variables.\n",
    "New features, such as 'isMerchant,' are created to capture specific patterns or characteristics in the data.\n",
    "Training and Evaluation:\n",
    "\n",
    "The dataset is split into training and testing sets to train the model on one subset and evaluate its performance on another unseen subset.\n",
    "The model is trained on the training set using the fit method and then used to make predictions on the testing set.\n",
    "Performance Metrics:\n",
    "\n",
    "Model performance is evaluated using various metrics, including:\n",
    "Accuracy: The proportion of correctly classified instances.\n",
    "Precision: The proportion of true positive predictions among all positive predictions.\n",
    "Recall (Sensitivity): The proportion of true positive predictions among all actual positive instances.\n",
    "F1 Score: The harmonic mean of precision and recall, providing a balance between the two.\n",
    "Scikit-Learn Library:\n",
    "\n",
    "The model is implemented using the RandomForestClassifier from the scikit-learn library, a popular machine learning library in Python.\n",
    "\n",
    "In summary, the Random Forest Classifier is a powerful and versatile model for fraud detection. Its ability to handle complex relationships in the data, mitigate overfitting, and provide feature importance makes it suitable for real-world applications where fraud patterns may be intricate and evolving. The choice of metrics for evaluation ensures a comprehensive understanding of the model's effectiveness in detecting fraudulent activities.\n",
    "\n",
    "### 3. How Did You Select Variables to be Included in the Model\n",
    "\n",
    "Variables were selected based on their relevance to fraud detection.\n",
    "Categorical variables were converted into numerical form using one-hot encoding.\n",
    "A new feature 'isMerchant' was created based on whether the recipient is a merchant or not.\n",
    "\n",
    "### 4. Demonstrate the Performance of the Model by Using the Best Set of Tools\n",
    "\n",
    "Performance metrics include Accuracy, Precision, Recall, and F1 Score.\n",
    "The model is trained on a training set and evaluated on a separate testing set.\n",
    "The RandomForestClassifier is used from the scikit-learn library.\n",
    "\n",
    "### 5. What Are the Key Factors that Predict Fraudulent Customer\n",
    "\n",
    "The model determines key factors based on feature importance derived from the Random Forest algorithm.\n",
    "Feature importance helps identify which variables have the most influence on predicting fraud.\n",
    "\n",
    "### 6. Do These Factors Make Sense? If Yes, How? If Not, How Not\n",
    "\n",
    "The factors are determined by the model based on patterns in the data.\n",
    "Understanding the exact interpretation might require domain knowledge, but feature importance provides insights into which variables contribute significantly to fraud prediction.\n",
    "\n",
    "### 7. What Kind of Prevention Should be Adopted While the Company Updates its Infrastructure\n",
    "\n",
    "Without specific details on the current infrastructure, general suggestions include implementing advanced anomaly detection, monitoring unusual patterns, and enhancing security measures.\n",
    "Regularly updating fraud prevention algorithms and collaborating with cybersecurity experts is crucial.\n",
    "\n",
    "### 8. Assuming These Actions Have Been Implemented, How Would You Determine if They Work\n",
    "\n",
    "Continuous monitoring of fraud incidents compared to pre-implementation statistics.\n",
    "Regular model evaluation and updating based on new data.\n",
    "Feedback from fraud detection teams and analysis of false positives and false negatives."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
